{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Programming_Session_5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZEg4IWrPY6QE",
        "RNTHnF-LkFFK",
        "sd9PcTFwkFwo",
        "I8J7ErjCVRJf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A84sSMXE3hbZ"
      },
      "source": [
        "\n",
        "# **<center>Machine Learning and Finance </center>**\n",
        "\n",
        "\n",
        "## <center> Programming Sessing 5  </center>\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://mlfbg.github.io/MachineLearningInFinance/\">\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1gmxxmwCR1WXK0IYtNqvE4QXFleznWqQO\" height=\"50\"/>\n",
        "    Course page</a>\n",
        "</td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1jmf8rBkH5nsRgyOMXvqeqvX0_o4Rzy48?usp=sharing\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" height=\"50\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Presentation of the Programming Session:\n"
      ],
      "metadata": {
        "id": "i3UNZ1BIqRBM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNbuq3vZIg-Q"
      },
      "source": [
        "\n",
        "\n",
        "In this programming session, we would like to implement the GloVe approach. It was introduced by Jeffrey Pennington, Richard Socher and  Christopher D. Manning in the paper: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
        "\n",
        "The programming session is subdivided into three parts:\n",
        "\n",
        "* In section 1, the objective is to load the data, preprocess it and create the **co-occurence matrix**. \n",
        "* In section 2, the objective is to train the model by using two different methods: **Gradient Descent** and **Alternating Least Squares**. \n",
        "* In section 3, the objective is to add a penalty term to the loss function as a **regularization** technique. \n",
        "\n",
        "**Notations:**\n",
        "\n",
        "* $\\mathcal{M}_{n,p}(\\mathbb{R})$ is the space of the matrices composed of n rows and p columns.\n",
        "\n",
        "* $I_n \\in \\mathcal{M}_{n,n}(\\mathbb{R})$ is the identity matrix of size n.\n",
        "\n",
        "* For all $z \\in \\mathbb{R}^D$, the $\\mathcal{L}^2$ norm on $\\mathbb{R}^D$ of $z$ is defined as follows: $||z||_2^2 = z^T z$\n",
        "\n",
        "*  For all $A = [a_{ij}]_{i,j} \\in \\mathcal{M}_{n,p}(\\mathbb{R})$ we define the Frobenius norm of $A$ as follows: $||A||_{\\text{F}}^2 = \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^p a_{ij}^2$ \n",
        "\n",
        "* The gradient of a function $f : \\theta \\in \\mathbb{R}^D \\mapsto \\mathbb{R}$ at $\\theta\\in \\mathbb{R}^D$ is denoted as follows $\\nabla_{\\theta}f(\\theta) = \\left(\\frac{\\partial f}{\\partial \\theta_1}(\\theta), \\dots, \\frac{\\partial f}{\\partial \\theta_D}(\\theta) \\right)$\n",
        "\n",
        "**Convention:** \n",
        "\t\n",
        "* The rows $(A_i)_{1 \\leq i \\leq n }$ of a matrix $A = \\begin{pmatrix}\n",
        "- & A_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & A_n & - \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{n,p}(\\mathbb{R}) $ are \t\t\t\tconsidered $\\mathcal{M}_{p,1}(\\mathbb{R})$ matrices. \n",
        "\n",
        "* The columns $(B_j)_{1 \\leq j \\leq p }$ of a matrix $B = \\begin{pmatrix}\n",
        "| & \\dots & | \\\\\n",
        "B_1 & \\dots & B_p \\\\\n",
        "| & \\dots & | \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{n,p}(\\mathbb{R}) $ are considered $\\mathcal{M}_{n,1}(\\mathbb{R})$ matrices. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS0eX7Kebja4",
        "outputId": "1780d0e7-409a-458b-d834-80be89f7364a"
      },
      "source": [
        "# Access files from Google Drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "os.chdir('./gdrive/My Drive/Colab Notebooks/Session_2022/Programming_Session_5/')"
      ],
      "metadata": {
        "id": "TXs9t3aPBEuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCcWOvm03Zxu"
      },
      "source": [
        "# Import basic libraries\n",
        "import pandas as pd # for dataframes\n",
        "import numpy as np # for arrays\n",
        "import matplotlib.pyplot as plt # for plots \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # for processing text\n",
        "import random # to shuffle the sequences\n",
        "import os \n",
        "plt.style.use('dark_background') # to adapt the colors to a dark background\n",
        "from IPython.display import Image # for showing graphs from the lectures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yDVjd7FbF9L"
      },
      "source": [
        "# Hyperparameters\n",
        "MAX_VOCAB = 999\n",
        "C = 10 # Context size\n",
        "V = MAX_VOCAB + 1 # Vocabulary size\n",
        "EPOCHS = 64\n",
        "D = 100 # Embedding dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEg4IWrPY6QE"
      },
      "source": [
        "# 1. Getting the statistics of the word occurences "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_myLbIjnJQx4"
      },
      "source": [
        "## 1.1 Introducing the problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pf_Enu1KgPy"
      },
      "source": [
        "The objective of the programming session is to train a model on a corpus of training sentences in order to represent words in a $D$-dimensional space. We would like to encode the similarity between the words in the embedding vectors themselves. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SutTRTQJT4J"
      },
      "source": [
        "\n",
        "---\n",
        "<font color=green>Q1:</font>\n",
        "<br><font color='green'>\n",
        "Explain why this notion of similarity is not encoded in the one hot vector representation of words.\n",
        "</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmoXLYiyKlfr"
      },
      "source": [
        "Several methods have been used to create word embeddings. The most popular ones rely on the intuition that *a word's meaning is given by the words that frequently appear close-by*. \n",
        "\n",
        "For instance, [the word2vec approach](https://arxiv.org/pdf/1301.3781.pdf) represents the tokens as parameters of a shallow neural network predicting a word's context given the world itself. \n",
        "\n",
        "Although the shallow window-based model captures linguistic patterns between word vectors and performs well on the word analogy task ($w_{\\text{France}} - w_{\\text{Paris}} \\approx w_{\\text{England}} - w_{\\text{London}}$), the model suffers from the disadvantage that they do not operate directly and the co-occurence statistics.\n",
        "\n",
        "As explained in [Lecture 5](https://mlfbg.github.io/MachineLearningInFinance/Lectures/Lecture_5.pdf), the GloVe method is a popular method used to learn low-dimensional word representations by using **matrix factorization** methods on a matrix of word-word **co-occurence** statistics. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn9LBGBIj9K6"
      },
      "source": [
        "## 1.2 Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG-5uQsmTf9L"
      },
      "source": [
        "The **data** folder contains a csv file named `RedditNews.csv` (Source: Sun, J. (2016, August) Daily News for Stock Market Prediction, Version 1. Retrieved [26 may 2020]).\n",
        "\n",
        "In the `RedditNews.csv` file are stored historical news headlines from Reddit WorldNews Channel, ranked by reddit users' votes, and only the top 25 headlines are considered for a single date.\n",
        "\n",
        "You will find two colomns: \n",
        "\n",
        "\n",
        "* The first column is for the \"date\".\n",
        "* The second column is for the \"News\". As all the news are ranked from top to bottom, there are only 25 lines for each date.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q2:</font>\n",
        "<br><font color='green'>\n",
        "Load the data from the csv file, create a list of all the news.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "k6wCO8hAF3Vo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-JxcrF8Khhs"
      },
      "source": [
        "# Import the data\n",
        "data = pd.read_csv(\"data/RedditNews.csv\")\n",
        "# Select the news\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IhGBiEn-BOTy",
        "outputId": "abd23db3-f987-48fc-ddfb-bc45667c0965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Date                                               News\n",
              "0      2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
              "1      2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
              "2      2016-07-01  The president of France says if Brexit won, so...\n",
              "3      2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
              "4      2016-07-01  100+ Nobel laureates urge Greenpeace to stop o...\n",
              "...           ...                                                ...\n",
              "73603  2008-06-08  b'Man goes berzerk in Akihabara and stabs ever...\n",
              "73604  2008-06-08  b'Threat of world AIDS pandemic among heterose...\n",
              "73605  2008-06-08  b'Angst in Ankara: Turkey Steers into a Danger...\n",
              "73606  2008-06-08  b\"UK: Identity cards 'could be used to spy on ...\n",
              "73607  2008-06-08  b'Marriage, they said, was reduced to the stat...\n",
              "\n",
              "[73608 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e1d9d35-4dc2-4aa8-80d7-a17bd2369301\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>News</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>The president of France says if Brexit won, so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73603</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Man goes berzerk in Akihabara and stabs ever...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73604</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Threat of world AIDS pandemic among heterose...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73605</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Angst in Ankara: Turkey Steers into a Danger...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73606</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b\"UK: Identity cards 'could be used to spy on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73607</th>\n",
              "      <td>2008-06-08</td>\n",
              "      <td>b'Marriage, they said, was reduced to the stat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>73608 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e1d9d35-4dc2-4aa8-80d7-a17bd2369301')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3e1d9d35-4dc2-4aa8-80d7-a17bd2369301 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3e1d9d35-4dc2-4aa8-80d7-a17bd2369301');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSiWG1LOTH8-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "<font color=green>Q3:</font>\n",
        "<br><font color='green'>\n",
        "Preprocess the data by transforming the list of sentences into a list of sequences of integers called `news_processed` , via a dictionary that maps the words to integers. \n",
        "</font>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=15_ocFu9iG0sOPmK0dKhECzTUbIXFSeHC\"></center>"
      ],
      "metadata": {
        "id": "s5grXRA_CgxV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qACCC4u1LHau"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q4:</font>\n",
        "<br><font color='green'>\n",
        "For each sentence, add a specific index for the token \"$<\\text{sos}>$\" (start of sequence) at the beginning of each sequence and an index for the token \"$<\\text{eos}>$\" (end of sequence) at the end of each sequence. The resulting list of lists of integers is called `sequences`. \n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gsuspkmgDvd3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UUGI0oNLWRk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37bxb3akkE4B"
      },
      "source": [
        "## 1.3 Creating the co-occurence matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMXi6HniTk6j"
      },
      "source": [
        "Let $V$ be the vocabulary size of the training corpus. \n",
        "\n",
        "In [Lecture 5](https://mlfbg.github.io/MachineLearningInFinance/Lectures/Lecture_5.pdf), we have defined the co-occurence matrix $X = [X_{ij}]_{i,j} \\in \\mathcal{M}_{V,V}(\\mathbb{R})$, whose entries $X_{ij}$ represent the number of times word $j$ appears in the context of word $i$. \n",
        "\n",
        "Algorithm 1 summarizes the steps involved in estimating the co-occurence matrix from the corpus `sequences`.\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=186c4b_X8mDEgBoVNZKEw7sfXOzKa-KN-\"></center>\n",
        "\n",
        "\n",
        "In Algorithm 1, each time a word $w[j]$ (of index $j$ in sequence) appears in the context of a center word $w[i]$ (of index $i$ in sequence), we increase the value of $X[w[i], w[j]]$ by a value of $1$ regardless of how close the word $w[j]$ is to the word $w[i]$. \n",
        "\n",
        "We would like to take into consideration the distance $d(i,j)$ between the center word $w[i]$ and the context word $w[j]$ when updating the value $X[w[i], w[j]]$, as shown the following figure:\n",
        "\n",
        "<center><img width=\"600\" src = \"https://drive.google.com/uc?export=view&id=1m1_32ovMfjkRVb-B_3gPizDI1QUfZjQ4\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q5:</font>\n",
        "<br><font color='green'>\n",
        "Explain why it makes more sense to use the following update equation for $X[w[i], w[j]]$ when word $w[j]$ of index $j$ is in the context word $w[i]$ of index $i$. \n",
        " \n",
        "\\begin{equation*}\n",
        "X[w[i], w[j]] \\longleftarrow X[w[i], w[j]] + \\frac{1}{|i-j|}\n",
        "\\end{equation*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "e1M45t0GD0xC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q6:</font>\n",
        "<br><font color='green'>\n",
        "Implement Algorithm 2 to get the co-occurence matrix $X$ using a function called `get_cooccurence_matrix()`.\n",
        "\n",
        "The function takes as arguments `sequences`, `context_size` and `vocabulary_size` and outputs the matrix `X`.\n",
        "</font>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=1Xt6SXVNxlsPHqIWk1IIaLOZcpTrQWZSd\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "XI-bKFPhD2Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Hlpk4oBmHfNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcufa32cNtgM",
        "outputId": "b8e00d31-a6d1-444c-dec5-9d8bebd60921"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get the co-occurence matrix X...\n",
            "number of sentences to process: 73608\n",
            "processed 10000 / 73608\n",
            "processed 20000 / 73608\n",
            "processed 30000 / 73608\n",
            "processed 40000 / 73608\n",
            "processed 50000 / 73608\n",
            "processed 60000 / 73608\n",
            "processed 70000 / 73608\n",
            "The shape of X is (1000, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zM372IuTZlr"
      },
      "source": [
        "\n",
        "\n",
        "Since non-zero values in the matrix $X$ are very large, we apply the logarithm function to all the elements of $X$ (after adding 1 to all the entries $X_{ij}$ to avoid applying the logarithm on zero values). The resulting matrix is still a sparse matrix. We will denote it $\\log X$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q7:</font>\n",
        "<br><font color='green'>\n",
        "Create the matrix $\\log X$, call it `logX`.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "U4c7UN5CD3ba"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ANrmOAOEb8",
        "outputId": "17ea38e8-091e-452f-f5f8-37fd5733a02a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get logX...\n",
            "The shape of logX is (1000, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNTHnF-LkFFK"
      },
      "source": [
        "# 2. Training the weighted least squares regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHIrgh9bkFM-"
      },
      "source": [
        "## 2.1 Introducing the cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r59PLdpZkFXS"
      },
      "source": [
        "The logarithm of the co-occurence matrix $\\log X$ has been defined in the previous section. The objective of this section is to approximate $\\log X$ using a factorization method as follows:\n",
        "\n",
        "\\begin{equation*}\n",
        "\t\\forall (i,j) \\in \\{1, \\dots, V \\}^2 \\quad \\log X_{ij} \\approx W_i^T \\tilde{W}_j + b_i + \\tilde{b}_j\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The parameters of the regression model are:\n",
        "\n",
        "\n",
        "* A first **embedding matrix** and a bias term associated with it:\n",
        "\t\\begin{equation*}\n",
        "W = \\begin{pmatrix}\n",
        "- & W_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & W_V & - \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{V, D}(\\mathbb{R}), \t\\quad  \tb = \\begin{pmatrix}\n",
        " b_1  \\\\\n",
        " \\vdots  \\\\\n",
        " b_V  \n",
        "\\end{pmatrix}\\in \\mathbb{R}^{V}\n",
        "\t\\end{equation*}\t\t\n",
        "\t\n",
        "\n",
        "* A second **embedding matrix** and a bias term associated with it:\n",
        "\t\\begin{equation*}\n",
        "\\tilde{W} = \\begin{pmatrix}\n",
        "- & \\tilde{W}_1 & - \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "- & \\tilde{W}_V & - \n",
        "\\end{pmatrix}\\in \\mathcal{M}_{V, D}(\\mathbb{R}), \\quad \\tilde{b} = \\begin{pmatrix}\n",
        " \\tilde{b}_1  \\\\\n",
        " \\vdots  \\\\\n",
        " \\tilde{b}_V  \n",
        "\\end{pmatrix}\\in \\mathbb{R}^{V} \t\n",
        "\t\\end{equation*}\t\n",
        "\n",
        "\n",
        "Instead of equal-weighting all the co-occurences, we introduce a **weighting function** $f(X_{ij})$ defined as follows: \n",
        "\n",
        "\\begin{equation*}\n",
        "\\forall x \\in \\mathbb{R}_{+} \\quad f(x) =  \\begin{cases}\n",
        "      (x/x_{\\text{max}})^{\\alpha} & \\text{if   $x < x_{\\text{max}}$}\\\\\n",
        "      1 & \\text{otherwise}\n",
        "          \\end{cases}  \n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The function $f$ is represented in the following figure with $x_{\\text{max}}=100$ and $\\alpha=0.75$\n",
        "\n",
        "<center><img width=\"400\" src = \"https://drive.google.com/uc?export=view&id=1D7muXkREj-5pPVUWyAfe8qrwYeIe0XzN\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q8:</font>\n",
        "<br><font color='green'>\n",
        "Create a matrix of shape $(V, V)$ whose entries are $f(X_{ij})$. \n",
        "Let's call it `fX`. Use the hyperparameters $x_{\\text{max}}=100$ and $\\alpha=0.75$\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dBlWZ8ZsD4qx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEnJbfO6ObLg",
        "outputId": "ee6230af-fd46-4624-d944-5e247404b241"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get f(X)...\n",
            "The shape of fX is (1000, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q9:</font>\n",
        "<br><font color='green'>\n",
        "What are the hyperparameters associated with the weighting function and what is the intuition behind introducing it?\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1ofPy91eD5UN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjWn9PfcTzqW"
      },
      "source": [
        "The **cost function** can then be written as follows: \n",
        "\n",
        "\\begin{equation*}\n",
        "J = \\sum_{i=1}^V \\sum_{j=1}^V f(X_{ij}) (\\log X_{ij} - W_i^T \\tilde{W}_j - b_i - \\tilde{b}_j)^2\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "The gradients of the cost function $J$ with respect to all the parameters are introduced in the following equations:\n",
        "\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$ and all $j \\in \\{ 1, \\dots, V \\}$:\n",
        "\n",
        "\n",
        "\\begin{align} \n",
        "& \\nabla_{W_i} J(W_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} \\quad \\text{(2.1)} \\\\\n",
        "& \\nabla_{\\tilde{W}_j} J(W_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) W_{i'} \\quad \\text{(2.2)}  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\quad \\text{(2.3)}  \\\\\n",
        "& \\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) \\quad \\text{(2.4)} \n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q10:</font>\n",
        "<br><font color='green'>\n",
        "What is the total number of parameters in the model ? What are the shapes of all the gradients introduced in the equations (2.1), (2.2), (2.3) and (2.4) ?\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XbzQWdyLD6Q2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rZEuCrPT3cH"
      },
      "source": [
        "Let us introduce two training methods:\n",
        "\n",
        "* The first training method is called **alternating least squares**. It consists in finding the update equations by setting all the gradients to zero.\n",
        "* The second training method consists in applying the **gradient descent** algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbPZ_vBQkFdg"
      },
      "source": [
        "## 2.2 Alternating least squares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-i1DbdJkFib"
      },
      "source": [
        "We would like to estimate the parameters $W, \\tilde{W}, b, \\tilde{b}$ by setting the gradients to zero. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "By setting the gradients to zero, we get the following update equations: \n",
        "\n",
        "\\begin{align*}\n",
        "&\\nabla_{W_i} J(W_i) = 0 \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right)  \\\\\n",
        "&\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 \\iff \\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = 0 \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)  \\\\\n",
        "&\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0 \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right) \n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "The proof can be found in **appendix A**"
      ],
      "metadata": {
        "id": "InI0dBGgD7tN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO0xwC4eXwAp"
      },
      "source": [
        "Each update equation for one parameter is a function of the other parameters. Therefore, in order to train our model, we can choose a number of iterations $N_{\\text{epochs}}$, and apply the update equations $N_{\\text{epochs}}$ times by keeping track of the loss to make sure it converges. \n",
        "\n",
        "For each iteration step $t \\in \\{0, \\dots, N_{\\text{epochs}}-1 \\}$, let $W^{(t)}, \\tilde{W}^{(t)}, b^{(t)}, \\tilde{b}^{(t)}$ represent the parameters of our model at the iteration $t$. \n",
        "\n",
        "\n",
        "The update equations from iteration $t$ to $t+1$ can then be written as follows:\n",
        "\n",
        "\\begin{align}\n",
        "&W_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'}^{(t)} \\tilde{W}_{j'}^{(t)^T} \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i^{(t)} - \\tilde{b}_{j'}^{(t)}) \\tilde{W}_{j'}^{(t)} \\right) \\quad \\text{(2.5)} \\\\\n",
        "&\\tilde{W}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'}^{(t)} W_{i'}^{(t)^T} \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'}^{(t)} - \\tilde{b}_{j}^{(t)}) W_{i'}^{(t)} \\right) \\quad \\text{(2.6)}  \\\\\n",
        "&b_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^{(t)^T} \\tilde{W}_{j'}^{(t)} - \\tilde{b}_{j'}^{(t)}) \\right) \\quad \\text{(2.7)} \\\\\n",
        "&\\tilde{b}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^{(t)^T} \\tilde{W}_{j}^{(t)} - b_{i'}^{(t)}) \\right) \\quad \\text{(2.8)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "The pseudo code for the training algorithm can be expressed as follows: \n",
        "\n",
        "\n",
        "<center><img width=\"500\" src = \"https://drive.google.com/uc?export=view&id=1DQmP3N13RH2hAP2-Szgk8TPhzcHICwGU\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q11:</font>\n",
        "<br><font color='green'>\n",
        "Implement the alternating least squares training algorithm\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "my9j51piD8-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62nQJ9gmaNk_",
        "outputId": "151bbba8-2564-4bd7-9949-6b2fd86480c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0...cost: 436200.10444292723\n",
            "Update W..\n",
            "Epoch 0... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 0... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 0... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 0... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 1...cost: 5158.165501315726\n",
            "Update W..\n",
            "Epoch 1... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 1... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 1... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 1... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 2...cost: 2784.9820458026015\n",
            "Update W..\n",
            "Epoch 2... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 2... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 2... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 2... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 3...cost: 2469.736208523399\n",
            "Update W..\n",
            "Epoch 3... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 3... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 3... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 3... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 4...cost: 2356.1766396214243\n",
            "Update W..\n",
            "Epoch 4... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 4... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 4... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 4... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 5...cost: 2300.429200508666\n",
            "Update W..\n",
            "Epoch 5... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 5... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 5... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 5... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 6...cost: 2268.316691840069\n",
            "Update W..\n",
            "Epoch 6... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 6... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 6... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 6... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 7...cost: 2247.6344010349176\n",
            "Update W..\n",
            "Epoch 7... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 7... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 7... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 7... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 8...cost: 2233.1341762557695\n",
            "Update W..\n",
            "Epoch 8... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 8... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 8... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 8... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 9...cost: 2222.294212710832\n",
            "Update W..\n",
            "Epoch 9... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 9... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 9... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 9... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 10...cost: 2213.8048734438707\n",
            "Update W..\n",
            "Epoch 10... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 10... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 10... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 10... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 11...cost: 2206.9419116873246\n",
            "Update W..\n",
            "Epoch 11... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 11... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 11... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 11... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 12...cost: 2201.278981205476\n",
            "Update W..\n",
            "Epoch 12... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 12... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 12... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 12... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 13...cost: 2196.544335373945\n",
            "Update W..\n",
            "Epoch 13... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 13... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 13... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 13... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 14...cost: 2192.547550654332\n",
            "Update W..\n",
            "Epoch 14... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 14... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 14... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 14... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 15...cost: 2189.1439784159725\n",
            "Update W..\n",
            "Epoch 15... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 15... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 15... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 15... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 16...cost: 2186.2188281881054\n",
            "Update W..\n",
            "Epoch 16... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 16... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 16... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 16... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 17...cost: 2183.6799960555604\n",
            "Update W..\n",
            "Epoch 17... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 17... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 17... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 17... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 18...cost: 2181.4540145578158\n",
            "Update W..\n",
            "Epoch 18... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 18... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 18... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 18... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 19...cost: 2179.48289032567\n",
            "Update W..\n",
            "Epoch 19... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 19... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 19... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 19... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 20...cost: 2177.7212464055224\n",
            "Update W..\n",
            "Epoch 20... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 20... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 20... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 20... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 21...cost: 2176.133754410729\n",
            "Update W..\n",
            "Epoch 21... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 21... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 21... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 21... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 22...cost: 2174.6929308734075\n",
            "Update W..\n",
            "Epoch 22... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 22... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 22... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 22... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 23...cost: 2173.37732088509\n",
            "Update W..\n",
            "Epoch 23... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 23... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 23... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 23... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 24...cost: 2172.170036719792\n",
            "Update W..\n",
            "Epoch 24... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 24... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 24... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 24... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 25...cost: 2171.057595618674\n",
            "Update W..\n",
            "Epoch 25... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 25... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 25... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 25... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 26...cost: 2170.029006345789\n",
            "Update W..\n",
            "Epoch 26... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 26... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 26... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 26... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 27...cost: 2169.075071568664\n",
            "Update W..\n",
            "Epoch 27... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 27... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 27... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 27... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 28...cost: 2168.187884166916\n",
            "Update W..\n",
            "Epoch 28... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 28... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 28... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 28... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 29...cost: 2167.3604923933945\n",
            "Update W..\n",
            "Epoch 29... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 29... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 29... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 29... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 30...cost: 2166.586698063053\n",
            "Update W..\n",
            "Epoch 30... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 30... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 30... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 30... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 31...cost: 2165.860945429568\n",
            "Update W..\n",
            "Epoch 31... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 31... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 31... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 31... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 32...cost: 2165.17826166425\n",
            "Update W..\n",
            "Epoch 32... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 32... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 32... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 32... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 33...cost: 2164.534220152768\n",
            "Update W..\n",
            "Epoch 33... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 33... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 33... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 33... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 34...cost: 2163.9249095130867\n",
            "Update W..\n",
            "Epoch 34... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 34... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 34... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 34... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 35...cost: 2163.346900451681\n",
            "Update W..\n",
            "Epoch 35... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 35... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 35... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 35... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 36...cost: 2162.797208270651\n",
            "Update W..\n",
            "Epoch 36... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 36... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 36... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 36... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 37...cost: 2162.273251656364\n",
            "Update W..\n",
            "Epoch 37... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 37... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 37... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 37... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 38...cost: 2161.7728093986807\n",
            "Update W..\n",
            "Epoch 38... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 38... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 38... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 38... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 39...cost: 2161.2939767914954\n",
            "Update W..\n",
            "Epoch 39... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 39... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 39... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 39... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 40...cost: 2160.835123196221\n",
            "Update W..\n",
            "Epoch 40... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 40... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 40... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 40... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 41...cost: 2160.394851889632\n",
            "Update W..\n",
            "Epoch 41... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 41... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 41... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 41... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 42...cost: 2159.971962982318\n",
            "Update W..\n",
            "Epoch 42... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 42... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 42... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 42... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 43...cost: 2159.5654199169485\n",
            "Update W..\n",
            "Epoch 43... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 43... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 43... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 43... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 44...cost: 2159.1743198334484\n",
            "Update W..\n",
            "Epoch 44... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 44... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 44... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 44... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 45...cost: 2158.797867907378\n",
            "Update W..\n",
            "Epoch 45... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 45... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 45... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 45... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 46...cost: 2158.435355615732\n",
            "Update W..\n",
            "Epoch 46... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 46... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 46... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 46... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 47...cost: 2158.0861427544755\n",
            "Update W..\n",
            "Epoch 47... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 47... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 47... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 47... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 48...cost: 2157.7496429243115\n",
            "Update W..\n",
            "Epoch 48... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 48... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 48... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 48... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 49...cost: 2157.425312118257\n",
            "Update W..\n",
            "Epoch 49... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 49... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 49... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 49... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 50...cost: 2157.1126399910877\n",
            "Update W..\n",
            "Epoch 50... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 50... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 50... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 50... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 51...cost: 2156.811143368326\n",
            "Update W..\n",
            "Epoch 51... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 51... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 51... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 51... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 52...cost: 2156.520361559713\n",
            "Update W..\n",
            "Epoch 52... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 52... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 52... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 52... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 53...cost: 2156.239853073971\n",
            "Update W..\n",
            "Epoch 53... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 53... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 53... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 53... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 54...cost: 2155.969193380974\n",
            "Update W..\n",
            "Epoch 54... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 54... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 54... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 54... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 55...cost: 2155.7079734258637\n",
            "Update W..\n",
            "Epoch 55... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 55... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 55... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 55... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 56...cost: 2155.4557986601935\n",
            "Update W..\n",
            "Epoch 56... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 56... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 56... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 56... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 57...cost: 2155.212288411941\n",
            "Update W..\n",
            "Epoch 57... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 57... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 57... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 57... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 58...cost: 2154.9770754656934\n",
            "Update W..\n",
            "Epoch 58... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 58... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 58... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 58... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 59...cost: 2154.7498057647767\n",
            "Update W..\n",
            "Epoch 59... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 59... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 59... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 59... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 60...cost: 2154.5301381781837\n",
            "Update W..\n",
            "Epoch 60... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 60... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 60... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 60... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 61...cost: 2154.3177442978504\n",
            "Update W..\n",
            "Epoch 61... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 61... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 61... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 61... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 62...cost: 2154.112308247255\n",
            "Update W..\n",
            "Epoch 62... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 62... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 62... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 62... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 63...cost: 2153.913526492215\n",
            "Update W..\n",
            "Epoch 63... W is updated for 0 words out of 1000\n",
            "Update b..\n",
            "Epoch 63... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 63... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 63... b_tilde is updated for 0 words out of 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q12:</font>\n",
        "<br><font color='green'>\n",
        " Plot the list of losses at the end of each iteration in Algorithm 3.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0cmFRYw-D95S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "L7br03pNbwRG",
        "outputId": "c23a163a-720b-4300-bb08-8373c6a9595c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX56bVMHkFnd"
      },
      "source": [
        "## 2.3 Learning the weights using gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luje3fIdkFsY"
      },
      "source": [
        "In this section, we would like to estimate the parameters of the model using gradient descent.\n",
        "\n",
        "Let $N_{\\text{epochs}}$ be the number of epochs and $\\eta$ be the learning rate. \n",
        "We get the following training algorithm:\n",
        "\n",
        "\n",
        "<center><img width=\"500\" src = \"https://drive.google.com/uc?export=view&id=1Od3xCvMWKOBhMpccmKOtoY3aT3UTJB5Z\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q13:</font>\n",
        "<br><font color='green'>\n",
        "Implement the gradient descent training algorithm (Algorithm 4).\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "85Ih7k7OD-py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY4e1IP3mKL4",
        "outputId": "1dcb1072-968f-410e-ea75-9ac6e2dd536f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0...cost: 436208.8015216822\n",
            "Update W..\n",
            "Epoch 0... W is updated for 0 words out of 1000\n",
            "Epoch 0... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 0... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 0... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 1...cost: 344889.34540995205\n",
            "Update W..\n",
            "Epoch 1... W is updated for 0 words out of 1000\n",
            "Epoch 1... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 1... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 1... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 2...cost: 278239.9444067379\n",
            "Update W..\n",
            "Epoch 2... W is updated for 0 words out of 1000\n",
            "Epoch 2... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 2... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 2... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 3...cost: 228959.31311879362\n",
            "Update W..\n",
            "Epoch 3... W is updated for 0 words out of 1000\n",
            "Epoch 3... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 3... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 3... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 4...cost: 192026.27889071166\n",
            "Update W..\n",
            "Epoch 4... W is updated for 0 words out of 1000\n",
            "Epoch 4... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 4... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 4... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 5...cost: 163960.47936508746\n",
            "Update W..\n",
            "Epoch 5... W is updated for 0 words out of 1000\n",
            "Epoch 5... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 5... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 5... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 6...cost: 142329.5951377474\n",
            "Update W..\n",
            "Epoch 6... W is updated for 0 words out of 1000\n",
            "Epoch 6... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 6... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 6... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 7...cost: 125419.13832119321\n",
            "Update W..\n",
            "Epoch 7... W is updated for 0 words out of 1000\n",
            "Epoch 7... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 7... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 7... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 8...cost: 112009.88713626136\n",
            "Update W..\n",
            "Epoch 8... W is updated for 0 words out of 1000\n",
            "Epoch 8... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 8... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 8... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 9...cost: 101226.94142982691\n",
            "Update W..\n",
            "Epoch 9... W is updated for 0 words out of 1000\n",
            "Epoch 9... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 9... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 9... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 10...cost: 92436.67532890647\n",
            "Update W..\n",
            "Epoch 10... W is updated for 0 words out of 1000\n",
            "Epoch 10... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 10... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 10... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 11...cost: 85175.89963106767\n",
            "Update W..\n",
            "Epoch 11... W is updated for 0 words out of 1000\n",
            "Epoch 11... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 11... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 11... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 12...cost: 79102.81364536095\n",
            "Update W..\n",
            "Epoch 12... W is updated for 0 words out of 1000\n",
            "Epoch 12... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 12... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 12... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 13...cost: 73962.79066751977\n",
            "Update W..\n",
            "Epoch 13... W is updated for 0 words out of 1000\n",
            "Epoch 13... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 13... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 13... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 14...cost: 69564.32906126912\n",
            "Update W..\n",
            "Epoch 14... W is updated for 0 words out of 1000\n",
            "Epoch 14... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 14... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 14... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 15...cost: 65762.01825158345\n",
            "Update W..\n",
            "Epoch 15... W is updated for 0 words out of 1000\n",
            "Epoch 15... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 15... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 15... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 16...cost: 62444.38006880087\n",
            "Update W..\n",
            "Epoch 16... W is updated for 0 words out of 1000\n",
            "Epoch 16... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 16... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 16... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 17...cost: 59525.12316809217\n",
            "Update W..\n",
            "Epoch 17... W is updated for 0 words out of 1000\n",
            "Epoch 17... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 17... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 17... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 18...cost: 56936.80443330097\n",
            "Update W..\n",
            "Epoch 18... W is updated for 0 words out of 1000\n",
            "Epoch 18... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 18... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 18... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 19...cost: 54626.20035480349\n",
            "Update W..\n",
            "Epoch 19... W is updated for 0 words out of 1000\n",
            "Epoch 19... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 19... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 19... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 20...cost: 52550.90209118477\n",
            "Update W..\n",
            "Epoch 20... W is updated for 0 words out of 1000\n",
            "Epoch 20... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 20... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 20... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 21...cost: 50676.792522346885\n",
            "Update W..\n",
            "Epoch 21... W is updated for 0 words out of 1000\n",
            "Epoch 21... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 21... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 21... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 22...cost: 48976.163493690554\n",
            "Update W..\n",
            "Epoch 22... W is updated for 0 words out of 1000\n",
            "Epoch 22... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 22... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 22... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 23...cost: 47426.300933519524\n",
            "Update W..\n",
            "Epoch 23... W is updated for 0 words out of 1000\n",
            "Epoch 23... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 23... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 23... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 24...cost: 46008.414191941716\n",
            "Update W..\n",
            "Epoch 24... W is updated for 0 words out of 1000\n",
            "Epoch 24... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 24... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 24... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 25...cost: 44706.820272427445\n",
            "Update W..\n",
            "Epoch 25... W is updated for 0 words out of 1000\n",
            "Epoch 25... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 25... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 25... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 26...cost: 43508.31800061004\n",
            "Update W..\n",
            "Epoch 26... W is updated for 0 words out of 1000\n",
            "Epoch 26... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 26... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 26... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 27...cost: 42401.70459970716\n",
            "Update W..\n",
            "Epoch 27... W is updated for 0 words out of 1000\n",
            "Epoch 27... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 27... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 27... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 28...cost: 41377.39968080518\n",
            "Update W..\n",
            "Epoch 28... W is updated for 0 words out of 1000\n",
            "Epoch 28... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 28... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 28... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 29...cost: 40427.15073638291\n",
            "Update W..\n",
            "Epoch 29... W is updated for 0 words out of 1000\n",
            "Epoch 29... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 29... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 29... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 30...cost: 39543.800840953416\n",
            "Update W..\n",
            "Epoch 30... W is updated for 0 words out of 1000\n",
            "Epoch 30... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 30... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 30... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 31...cost: 38721.10411064324\n",
            "Update W..\n",
            "Epoch 31... W is updated for 0 words out of 1000\n",
            "Epoch 31... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 31... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 31... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 32...cost: 37953.578045967064\n",
            "Update W..\n",
            "Epoch 32... W is updated for 0 words out of 1000\n",
            "Epoch 32... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 32... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 32... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 33...cost: 37236.38452858131\n",
            "Update W..\n",
            "Epoch 33... W is updated for 0 words out of 1000\n",
            "Epoch 33... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 33... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 33... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 34...cost: 36565.23321338447\n",
            "Update W..\n",
            "Epoch 34... W is updated for 0 words out of 1000\n",
            "Epoch 34... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 34... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 34... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 35...cost: 35936.3025317955\n",
            "Update W..\n",
            "Epoch 35... W is updated for 0 words out of 1000\n",
            "Epoch 35... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 35... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 35... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 36...cost: 35346.17463050524\n",
            "Update W..\n",
            "Epoch 36... W is updated for 0 words out of 1000\n",
            "Epoch 36... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 36... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 36... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 37...cost: 34791.7814071464\n",
            "Update W..\n",
            "Epoch 37... W is updated for 0 words out of 1000\n",
            "Epoch 37... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 37... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 37... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 38...cost: 34270.359439416716\n",
            "Update W..\n",
            "Epoch 38... W is updated for 0 words out of 1000\n",
            "Epoch 38... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 38... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 38... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 39...cost: 33779.412088143166\n",
            "Update W..\n",
            "Epoch 39... W is updated for 0 words out of 1000\n",
            "Epoch 39... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 39... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 39... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 40...cost: 33316.67742518911\n",
            "Update W..\n",
            "Epoch 40... W is updated for 0 words out of 1000\n",
            "Epoch 40... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 40... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 40... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 41...cost: 32880.10092188838\n",
            "Update W..\n",
            "Epoch 41... W is updated for 0 words out of 1000\n",
            "Epoch 41... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 41... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 41... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 42...cost: 32467.81205361967\n",
            "Update W..\n",
            "Epoch 42... W is updated for 0 words out of 1000\n",
            "Epoch 42... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 42... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 42... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 43...cost: 32078.104146763\n",
            "Update W..\n",
            "Epoch 43... W is updated for 0 words out of 1000\n",
            "Epoch 43... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 43... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 43... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 44...cost: 31709.416927276583\n",
            "Update W..\n",
            "Epoch 44... W is updated for 0 words out of 1000\n",
            "Epoch 44... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 44... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 44... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 45...cost: 31360.321334297238\n",
            "Update W..\n",
            "Epoch 45... W is updated for 0 words out of 1000\n",
            "Epoch 45... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 45... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 45... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 46...cost: 31029.506244152126\n",
            "Update W..\n",
            "Epoch 46... W is updated for 0 words out of 1000\n",
            "Epoch 46... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 46... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 46... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 47...cost: 30715.766815023217\n",
            "Update W..\n",
            "Epoch 47... W is updated for 0 words out of 1000\n",
            "Epoch 47... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 47... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 47... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 48...cost: 30417.994214076756\n",
            "Update W..\n",
            "Epoch 48... W is updated for 0 words out of 1000\n",
            "Epoch 48... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 48... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 48... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 49...cost: 30135.16653009238\n",
            "Update W..\n",
            "Epoch 49... W is updated for 0 words out of 1000\n",
            "Epoch 49... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 49... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 49... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 50...cost: 29866.340707760424\n",
            "Update W..\n",
            "Epoch 50... W is updated for 0 words out of 1000\n",
            "Epoch 50... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 50... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 50... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 51...cost: 29610.645366589608\n",
            "Update W..\n",
            "Epoch 51... W is updated for 0 words out of 1000\n",
            "Epoch 51... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 51... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 51... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 52...cost: 29367.274389126513\n",
            "Update W..\n",
            "Epoch 52... W is updated for 0 words out of 1000\n",
            "Epoch 52... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 52... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 52... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 53...cost: 29135.48118096683\n",
            "Update W..\n",
            "Epoch 53... W is updated for 0 words out of 1000\n",
            "Epoch 53... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 53... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 53... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 54...cost: 28914.5735196483\n",
            "Update W..\n",
            "Epoch 54... W is updated for 0 words out of 1000\n",
            "Epoch 54... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 54... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 54... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 55...cost: 28703.908921586204\n",
            "Update W..\n",
            "Epoch 55... W is updated for 0 words out of 1000\n",
            "Epoch 55... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 55... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 55... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 56...cost: 28502.89046624031\n",
            "Update W..\n",
            "Epoch 56... W is updated for 0 words out of 1000\n",
            "Epoch 56... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 56... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 56... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 57...cost: 28310.963025077468\n",
            "Update W..\n",
            "Epoch 57... W is updated for 0 words out of 1000\n",
            "Epoch 57... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 57... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 57... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 58...cost: 28127.60984992634\n",
            "Update W..\n",
            "Epoch 58... W is updated for 0 words out of 1000\n",
            "Epoch 58... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 58... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 58... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 59...cost: 27952.349481253317\n",
            "Update W..\n",
            "Epoch 59... W is updated for 0 words out of 1000\n",
            "Epoch 59... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 59... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 59... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 60...cost: 27784.732941919752\n",
            "Update W..\n",
            "Epoch 60... W is updated for 0 words out of 1000\n",
            "Epoch 60... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 60... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 60... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 61...cost: 27624.341186267342\n",
            "Update W..\n",
            "Epoch 61... W is updated for 0 words out of 1000\n",
            "Epoch 61... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 61... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 61... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 62...cost: 27470.78277804382\n",
            "Update W..\n",
            "Epoch 62... W is updated for 0 words out of 1000\n",
            "Epoch 62... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 62... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 62... b_tilde is updated for 0 words out of 1000\n",
            "epoch: 63...cost: 27323.691773833703\n",
            "Update W..\n",
            "Epoch 63... W is updated for 0 words out of 1000\n",
            "Epoch 63... b is updated for 0 words out of 1000\n",
            "Update W_tilde..\n",
            "Epoch 63... W_tilde is updated for 0 words out of 1000\n",
            "Update b_tilde..\n",
            "Epoch 63... b_tilde is updated for 0 words out of 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q14:</font>\n",
        "<br><font color='green'>\n",
        " Plot the list of losses at the end of each iteration in Algorithm 4.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6aS7uO42D_Ya"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VRIWLWrmfnR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "e78a511e-a850-4484-fab9-0a4fcf3a9886"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd9PcTFwkFwo"
      },
      "source": [
        "# 3. Exercise: Introducing regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWkYPM1dkF1u"
      },
      "source": [
        "Let us introduce a regularization penalty term in the cost function. The new cost function is defined as follows:\n",
        "\n",
        "\n",
        "\\begin{equation*}\n",
        "\\tilde{J} = \\sum\\limits_{i=1}^V \\sum\\limits_{j=1}^V f(X_{ij}) (\\log X_{ij} - W_i^T \\tilde{W}_j - b_i - \\tilde{b}_j)^2 + \\lambda \\left( ||W||_{\\text{F}}^2 +   ||\\tilde{W}||_{\\text{F}}^2 + ||b||_2^2 + ||\\tilde{b} ||_2^2 \\right) \n",
        "\\end{equation*}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q15:</font>\n",
        "<br><font color='green'>\n",
        "Show that: \n",
        "\\begin{equation}\n",
        "||W||_{\\text{F}}^2 = \\sum\\limits_{i=1}^V W_i^T W_i\n",
        "\\end{equation}\n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0Fgcb-ASEAad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q16:</font>\n",
        "<br><font color='green'>\n",
        "Deduce that for all $i \\in \\{1, \\dots, V \\}$:\n",
        "\\begin{align} \n",
        "& \\nabla_{W_i} (||W||_{\\text{F}}^2) = 2W_i \\quad \\text{(3.1)} \\\\ \n",
        "&(\\text{Hint}: \\forall z \\in \\mathbb{R}^D \\ \\forall A \\in \\mathcal{M}_{D, D}(\\mathbb{R}) \\quad \\nabla_z (z^T A z) = (A + A^T)z )\n",
        "\\end{align} \n",
        "\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JgdEJEcFEBCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q17:</font>\n",
        "<br><font color='green'>\n",
        "From the equations(2.1), (2.2), (2.3), (2.4) and (3.1), show that the update equations for the method of alternating least squares become: \n",
        "\n",
        "\\begin{align*}\n",
        "&W_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'}^{(t)} \\tilde{W}_{j'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i^{(t)} - \\tilde{b}_{j'}^{(t)}) \\tilde{W}_{j'}^{(t)} \\right)  \\\\\n",
        "&\\tilde{W}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'}^{(t)} W_{i'}^{(t)^T} + \\lambda I_D \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'}^{(t)} - \\tilde{b}_{j}^{(t)}) W_{i'}^{(t)} \\right) \\\\\n",
        "&b_i^{(t+1)} \\longleftarrow \\left( \\sum_{j'=1}^V f(X_{ij'}) + \\lambda  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^{(t)^T} \\tilde{W}_{j'}^{(t)} - \\tilde{b}_{j'}^{(t)}) \\right)  \\\\\n",
        "&\\tilde{b}_j^{(t+1)} \\longleftarrow \\left( \\sum_{i'=1}^V f(X_{i' j}) + \\lambda  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^{(t)^T} \\tilde{W}_{j}^{(t)} - b_{i'}^{(t)}) \\right) \n",
        "\\end{align*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HcfbBXP2EBpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<font color=green>Q18:</font>\n",
        "<br><font color='green'>\n",
        " What would be the update equations for minimizing the new loss function $\\tilde{J}$ by using the gradient descent algorithm.\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OcsjfGD9EC2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix "
      ],
      "metadata": {
        "id": "I8J7ErjCVRJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<br><font color='green'>\n",
        "Let us show that:\n",
        "\n",
        "\\begin{align*}\n",
        "&\\nabla_{W_i} J(W_i) = 0 \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right)  \\\\\n",
        "&\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 \\iff \\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)  \\\\\n",
        "&\\nabla_{b_i} J(b_i) = 0 \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)  \\\\\n",
        "&\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0 \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right) \n",
        "\\end{align*}\n",
        "</font>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZxPxjvefVel3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- \n",
        "**Proof:**\n",
        "\n",
        "First, we need to prove a preliminary result:\n",
        "\n",
        "\\begin{equation*}\n",
        "  \\forall a,b \\in \\mathcal{M}_{D, 1}(\\mathbb{R}) \\quad (a^T b) \\ b = (b \\ b^T) \\ a \\quad (\\Delta)\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "Indeed, \n",
        "\n",
        "\\begin{align}\n",
        "\\forall a,b \\in \\mathcal{M}_{D, 1}(\\mathbb{R}) \\quad (a^T b) \\ b  &= b \\ (a^T b) \\\\\n",
        "&= b \\ (b^T a) \\quad (\\text{As} \\ a^Tb \\ \\text{is a scalar, it's equal to its transpose}) \\\\\n",
        "&=  (b b^T) \\ a \n",
        "\\end{align}\n",
        "\n",
        "For all $i \\in \\{1, \\dots, V \\}$ and for all $j \\in \\{1, \\dots, V \\}$. \n",
        "\n",
        "Let us find the optimal parameters by setting the gradients to 0:\n",
        "\n",
        "* For $W_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{W_i} J(W_i) = 0 & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = 0 \\quad \\text{(From (2.1))} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\sum_{j'=1}^V f(X_{ij'})  W_i^T \\tilde{W}_{j'} \\tilde{W}_{j'} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - b_i - \\tilde{b}_{j'} \\right) \\tilde{W}_{j'} = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right) W_i  \\quad (\\text{From} \\ (\\Delta)) \\\\\n",
        "& \\iff W_i = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\tilde{W}_{j'} \\tilde{W}_{j'}^T \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - b_i - \\tilde{b}_{j'}) \\tilde{W}_{j'} \\right) \n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{W}_{j}$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{W}_j} J(\\tilde{W}_j) = 0 & \\iff  -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) = 0  \\quad \\text{(From (2.2))} \\\\\n",
        "&\\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\sum_{i'=1}^V f(X_{i' j})  W_{i'}^T \\tilde{W}_j  W_{i'} \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - b_{i'} - \\tilde{b}_j \\right) W_{i'} = \\left( \\sum_{i'=1}^V f(X_{i' j})  W_{i'} W_{i'}^T \\right)  \\tilde{W}_j    \\\\\n",
        "& \\iff\\tilde{W}_j = \\left( \\sum_{i'=1}^V f(X_{i' j}) W_{i'} W_{i'}^T \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - b_{i'} - \\tilde{b}_{j}) W_{i'} \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $b_i$:\n",
        "\\begin{align*}\n",
        "\\nabla_{b_i} J(b_i) = 0  & \\iff -2 \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - b_i - \\tilde{b}_{j'} \\right) = 0 \\quad \\text{(From (2.3))} \\\\\n",
        "& \\iff \\sum_{j'=1}^V f(X_{ij'}) \\left( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'} \\right)  = \\left( \\sum_{j'=1}^V f(X_{ij'}) \\right) b_i \\\\\n",
        "& \\iff b_i = \\left( \\sum_{j'=1}^V f(X_{ij'})  \\right)^{-1} \\left( \\sum_{j'=1}^{V} f(X_{ij'})( \\log X_{ij'} - W_i^T \\tilde{W}_{j'} - \\tilde{b}_{j'}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "* For $\\tilde{b}_j$:\n",
        "\\begin{align*}\n",
        "\\nabla_{\\tilde{b}_j} J(\\tilde{b}_j) = 0  & \\iff -2 \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'} - \\tilde{b}_j \\right) = 0 \\quad \\text{(From (2.4))} \\\\\n",
        "& \\iff \\sum_{i'=1}^V f(X_{i' j}) \\left( \\log X_{i' j} - W_{i'}^T \\tilde{W}_j - b_{i'}   \\right) = \\left(\\sum_{i'=1}^V f(X_{i' j}) \\right) \\tilde{b}_j \\\\\n",
        "& \\iff \\tilde{b}_j = \\left( \\sum_{i'=1}^V f(X_{i' j})  \\right)^{-1} \\left( \\sum_{i'=1}^{V} f(X_{i' j})( \\log X_{i' j} - W_{i'}^T \\tilde{W}_{j} - b_{i'}) \\right) \n",
        "\\end{align*}\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Fs8YgSFKXIHP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_I-tM9dK3dY"
      },
      "source": [
        "# Contact\n",
        "\n",
        "If you have any question regarding this notebook, do not hesitate to contact: h.madmoun@imperial.ac.uk\n",
        "\n"
      ]
    }
  ]
}